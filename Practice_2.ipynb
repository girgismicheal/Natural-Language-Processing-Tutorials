{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice No.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzKrHImIp31S"
      },
      "outputs": [],
      "source": [
        "### Reference: DataCamp intro to NLP in Python\n",
        "### Building a Counter with bag-of-words ###\n",
        "# Import Counter\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Read sample wiki text file\n",
        "with open('wiki_text_computer.txt') as f:\n",
        "  article = f.read()\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Text preprocessing practice ### VERY IMPORTANT\n",
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read English stopwords text file\n",
        "with open('english_stopwords.txt') as f:\n",
        "  english_stops = f.read()\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))"
      ],
      "metadata": {
        "id": "nkC7YBzWrCXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating and querying a corpus with gensim ###\n",
        "# create your gensim dictionary and corpus VERY IMPORTANT! Please fill in the blanks\n",
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "# XXX is a list of document tokens storing words in your selected text, e.g., a \n",
        "# list contains five sub-lists of tokens in five books.\n",
        "# This list should be preprocessed by lowercasing all words, tokenizing them, and \n",
        "# removing stop words and punctuation, etc. before calling Dictionary.\n",
        "dictionary = Dictionary(XXX)\n",
        "\n",
        "# Select the id for \"yyy\" (i.e., any token): computer_id\n",
        "yyy_id = dictionary.token2id.get(\"yyyy\")\n",
        "\n",
        "# Use computer_id with the dictionary to print the word\n",
        "print(dictionary.get(yyyr_id))\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(x) for x in XXX]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "print(corpus[4][:10])"
      ],
      "metadata": {
        "id": "J9LmBjm7rr30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}