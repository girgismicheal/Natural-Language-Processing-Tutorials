# -*- coding: utf-8 -*-
"""Practice No.2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vSiL5i84OFWASNJKADOSheWbEwpQ7xJ7
"""

### Reference: DataCamp intro to NLP in Python
### Building a Counter with bag-of-words ###
# Import Counter
from collections import Counter
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.tokenize import word_tokenize

# Read sample wiki text file
with open('wiki_text_computer.txt') as f:
  article = f.read()

# Tokenize the article: tokens
tokens = word_tokenize(article)

# Convert the tokens into lowercase: lower_tokens
lower_tokens = [t.lower() for t in tokens]

# Create a Counter with the lowercase tokens: bow_simple
bow_simple = Counter(lower_tokens)

# Print the 10 most common tokens
print(bow_simple.most_common(10))

### Text preprocessing practice ### VERY IMPORTANT
# Import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

# Read English stopwords text file
with open('english_stopwords.txt') as f:
  english_stops = f.read()

# Retain alphabetic words: alpha_only
alpha_only = [t for t in lower_tokens if t.isalpha()]

# Remove all stop words: no_stops
no_stops = [t for t in alpha_only if t not in english_stops]

# Instantiate the WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

# Lemmatize all tokens into a new list: lemmatized
lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]

# Create the bag-of-words: bow
bow = Counter(lemmatized)

# Print the 10 most common tokens
print(bow.most_common(10))

### Creating and querying a corpus with gensim ###
# create your gensim dictionary and corpus VERY IMPORTANT! Please fill in the blanks
# Import Dictionary
from gensim.corpora.dictionary import Dictionary

# Create a Dictionary from the articles: dictionary
# XXX is a list of document tokens storing words in your selected text, e.g., a 
# list contains five sub-lists of tokens in five books.
# This list should be preprocessed by lowercasing all words, tokenizing them, and 
# removing stop words and punctuation, etc. before calling Dictionary.
dictionary = Dictionary(XXX)

# Select the id for "yyy" (i.e., any token): computer_id
yyy_id = dictionary.token2id.get("yyyy")

# Use computer_id with the dictionary to print the word
print(dictionary.get(yyyr_id))

# Create a MmCorpus: corpus
corpus = [dictionary.doc2bow(x) for x in XXX]

# Print the first 10 word ids with their frequency counts from the fifth document
print(corpus[4][:10])